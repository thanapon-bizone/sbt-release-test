# Includes steps for deploying libraries to DBFS in different environments
#
# Needed parameters
#     azureSubscription: service connection for the subcription
#     keyVaultName: name of the key vault
#     applicationId: app ID for the DevOps SP
#     resourceGroup: resource group where the databricks workspace resides
#     workspace: name of the databricks workspace
#     subscriptionId: subscription ID of the databricks workspace
#     tenantId: AD tenant ID
#     databricksInstance: Full URL of the databricks workspace
#     databricksPath: DBFS batch for jars to be deployed
#     environment: Deployment environment. Dev, Test, Prod etc
#     versionBump: Version information. Required only for production deployments
#
stages:

- stage:  '${{parameters.stageName}}'
  condition: eq(${{parameters.shouldRun}}, 'true')
  displayName: '${{parameters.stageDisplayName}}'
  jobs:
    - deployment: '${{parameters.stageName}}'
      displayName: '${{parameters.stageDisplayName}}'
      environment: '${{parameters.environment}}'
      pool:
        vmImage: $(vmImage)
      strategy:
        runOnce:
          deploy:
            steps:

              - bash: |
                  echo "Pipeline triggered by the branch ${{ parameters.branchName }}"
                  echo "isFeatureBranch: ${{ parameters.isFeatureBranch }}"
                displayName: 'Log run branch details'
                
              - task: UsePythonVersion@0
                inputs:
                  versionSpec: '3.x'
                  addToPath: true
                  architecture: 'x64'
                displayName: 'Use Python3'

              #### ============= Use AD Auth =================#
              # - task: AzureCLI@2
              #   continueOnError: false
              #   displayName: 'Get Databricks Token'
              #   inputs:
              #     azureSubscription: '${{parameters.azureSubscription}}'
              #     scriptType: 'bash'
              #     scriptLocation : 'inlineScript'
              #     inlineScript: |
              #       token=$(az account get-access-token --resource $(databricksEnterpriseId) --query 'accessToken')
              #       echo "##vso[task.setvariable variable=databrickstoken;issecret=true]$token"
              # - task: ShellScript@2
              #   inputs:
              #     scriptPath: $(Pipeline.Workspace)/shell-scripts/configureDatabricks.sh
              #     args: ${{ parameters.databricksInstance }} $(databrickstoken)
              #   displayName: 'ShellScript $ configureDatabricks.sh'
              #### ============= Use AD Auth =================#

              - template: 'step-keyvault.yml'
                parameters:
                  connectedServiceName: '${{parameters.devOpsServiceConnection}}'
                  keyVault: '${{parameters.keyVaultName}}'
                  kvAccessTokenName: 'devops-databricks-access-token'

              - task: ShellScript@2
                inputs:
                  scriptPath: $(Pipeline.Workspace)/shell-scripts/configureDatabricks.sh
                  args: ${{ parameters.databricksInstance }} $(devops-databricks-access-token)
                displayName: 'ShellScript $ configureDatabricks.sh'

              - checkout: self
                persistCredentials: true
              - script: |
                  git fetch
                  git checkout ${{ parameters.branchName }}
                  echo "Working on branch ${{ parameters.branchName }}"
                  echo "user.email $(Build.RequestedForEmail)"
                  echo "user.name $(Build.RequestedFor)"
                  git config --global user.email "$(Build.RequestedForEmail)"
                  git config --global user.name "$(Build.RequestedFor)"
                displayName: 'Git setup'

              # Run the following only in prod
              - ${{ if contains(parameters.environment, 'prod') }}:
                - script: (cd $(velocitylibpath)/; sbt "set releaseVersionBump := sbtrelease.Version.Bump.${{ parameters.versionBump }}; release with-defaults")
                  displayName: 'Release update version'
                # - script: |
                #     git -c http.extraheader="AUTHORIZATION: bearer $SYSTEM_ACCESS_TOKEN" push origin --follow-tags
                #   displayName: 'Tag version'
                #   env:
                #     SYSTEM_ACCESS_TOKEN: $(System.AccessToken)
                - bash: |
                    tagVersion=$(git describe --tags --abbrev=0)
                    tagReleaseVersion="${tagVersion}"
                    echo "Package version tag: $tagReleaseVersion"
                    echo "##vso[task.setvariable variable=packageversion]$tagReleaseVersion"
                  displayName: 'Get release version'
              # Run the following in other environments
              - ${{ else }}:
                - bash: |
                    tagVersion=$(git describe --tags --abbrev=0)
                    tagFeatureVersion="${tagVersion}-FEATURE"
                    echo "Package version tag: $tagFeatureVersion"
                    echo "##vso[task.setvariable variable=packageversion]$tagFeatureVersion"
                  displayName: 'Get release version'


              # Run the following on feature branch changes
              - ${{ if eq(parameters.isFeatureBranch, true) }}:
                - task: Bash@3
                  inputs:
                    targetType: 'filePath'
                    filePath: $(Pipeline.Workspace)/shell-scripts/copyToDBFS.sh
                    arguments: $(packagename) $(packageversion) $(scalaversion) ${{ parameters.databricksPath }}/branches/${{ parameters.branchName }}
                    workingDirectory: $(Pipeline.Workspace)
                  displayName: 'Bash $ copyToDBFS.sh'
              # Run the following on the main branch changes
              - ${{ else }}:
                - task: Bash@3
                  inputs:
                    targetType: 'filePath'
                    filePath: $(Pipeline.Workspace)/shell-scripts/copyToDBFS.sh
                    arguments: $(packagename) $(packageversion) $(scalaversion) ${{ parameters.databricksPath }}/main
                    workingDirectory: $(Pipeline.Workspace)
                  displayName: 'Bash $ copyToDBFS.sh'